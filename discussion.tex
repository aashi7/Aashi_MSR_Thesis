We return to the question posed in introduction, 'Is it possible to predict the time to near-collision from a single camera?'. The answer is that the proposed model is able to leverage spatio-temporal cues for predicting the time to collision within $0.75 \pm 0.68$ seconds on the test videos. Also, we observed that the multi-stream network of shared weights performed the task of collision forecasting better than I3D on the proposed dataset. 
%One of the major boosts to multi-stream VGG is the reuse of VGG-16 network pretrained on PASCAL VOC classification dataset which contains cimages of person class whereas I3D is pretrained Kinetics Human Action Video. 

With regard to temporal window of input history, it is evident that using a sequence of images has a considerable benefit over prediction from single frame only. Though the history of 0.5 seconds performed best, we do not observe a piecewise monotonic relation between input frames and error in prediction. This observation aligns with the performance of constant velocity model where accuracy in prediction does not necessarily increase or decrease with the temporal footprint of past trajectory. 

We understand that the proposed model might not generalize well when there are large changes in camera's height or structure of the scenes in comparison to provided dataset. In the scenarios where assistive robot operates in a constrained domain like museums and airports, the proposed approach will be suitable for predicting time to collision requiring only a low-cost monocular camera.